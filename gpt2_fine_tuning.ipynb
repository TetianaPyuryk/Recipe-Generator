{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7DtFZh-KIE2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zTIjPigCiGd"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='new_processed_recipes_data.txt',\n",
        "    block_size=256\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./gpt2-fine-tuned\")\n",
        "tokenizer.save_pretrained('./gpt2-tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfAePgtmSXt3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('./gpt2-fine-tuned')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2-tokenizer')\n",
        "\n",
        "prompt = \"[Q] Ingredients: potatoes, butter, milk, salt, pepper\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "output = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id, top_k=70, top_p=0.9)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/gpt2-fine-tuned.zip ./gpt2-fine-tuned\n",
        "!zip -r /content/gpt2-tokenizer.zip ./gpt2-tokenizer"
      ],
      "metadata": {
        "id": "6lR5x-CjYP4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"./gpt2-fine-tuned.zip\")\n",
        "files.download(\"./gpt2-tokenizer.zip\")"
      ],
      "metadata": {
        "id": "p04dVhwNYTuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}